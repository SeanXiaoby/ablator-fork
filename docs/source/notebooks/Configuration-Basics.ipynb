{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ablator framework uses a configuration system to define everything related to the training of machine learning models, from the model architecture, to the environment that it's being trained in.\n",
    "\n",
    "Ablator has the ability to dynamically create a hierarchical configuration by composition, and you can either override it through `yaml` config files and the command line, or you can just play around with python objects and classes. Refer to [these examples](./GettingStarted-more-demos.ipynb) or [the last two sections](#different-methods-to-define-running-configurations) in this tutorial to see how you can implement these two methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration categories\n",
    "\n",
    "For our framework, configuration is organized into different categories:\n",
    "- Running configuration (either for training a single model or training multiple models in parallel)\n",
    "- Model configuration\n",
    "- Training configuration\n",
    "- Optimizer configuration\n",
    "- Scheduler configuration.\n",
    "\n",
    "Most of them will be used together in order for `ablator` to work seamlessly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunConfig\n",
    "\n",
    "`RunConfig` is used to configure the experiment environment, e.g where to store experiment artifacts (loss, accuracy, other evaluation metrics), the device to be used (GPU, CPU), when to do validation step or progress logging while running the experiment.\n",
    "\n",
    "The table below summarizes the parameters, either required or customizable. Note that `RunConfig` requires `TrainConfig` and `ModelConfig` to be included during initialization, which are covered in the next sections of this tutorial.\n",
    "\n",
    "| Parameter           | Usage                                                                                                                                                                                                                                                                                                             |\n",
    "|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| experiment_dir      | location to store experiment artifacts.                                                                                                                                                                                                                                                                           |\n",
    "| random_seed         | random seed.                                                                                                                                                                                                                                                                                                      |\n",
    "| train_config        | training configuration. (check ``TrainConfig`` for more details)                                                                                                                                                                                                                                                  |\n",
    "| model_config        | model configuration. (check ``ModelConfig`` for more details)                                                                                                                                                                                                                                                     |\n",
    "| keep_n_checkpoints  | number of latest checkpoints to keep.                                                                                                                                                                                                                                                                             |\n",
    "| tensorboard         | whether to use tensorboardLogger.                                                                                                                                                                                                                                                                                 |\n",
    "| amp                 | whether to use automatic mixed precision when running on gpu.                                                                                                                                                                                                                                                     |\n",
    "| device              | device to run on.                                                                                                                                                                                                                                                                                                 |\n",
    "| verbose             | verbosity level.                                                                                                                                                                                                                                                                                                  |\n",
    "| eval_subsample      | fraction of the dataset to use for evaluation.                                                                                                                                                                                                                                                                    |\n",
    "| metrics_n_batches   | max number of batches stored in every tag(train, eval, test) for evaluation.                                                                                                                                                                                                                                      |\n",
    "| metrics_mb_limit    | max number of megabytes stored in every tag(train, eval, test) for evaluation.                                                                                                                                                                                                                                    |\n",
    "| early_stopping_iter | The maximum allowed difference between the current iteration and the last <br />iteration with the best metric before applying early stopping. Early stopping <br />will be triggered if the difference ``(current_itr-best_itr)`` exceeds ``early_stopping_iter``.<br />If set to ``None``, early stopping will not be applied. |\n",
    "| eval_epoch          | The epoch interval between two evaluations.                                                                                                                                                                                                                                                                       |\n",
    "| log_epoch           | The epoch interval between two logging.                                                                                                                                                                                                                                                                           |\n",
    "| init_chkpt          | path to a checkpoint to initialize the model with.                                                                                                                                                                                                                                                                |\n",
    "| warm_up_epochs      | number of epochs marked as warm up epochs.                                                                                                                                                                                                                                                                        |\n",
    "| divergence_factor   | if ``cur_loss > best_loss > divergence_factor``, the model is considered <br />to have diverged.                                                                                                                                                                                                                        |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParallelConfig\n",
    "\n",
    "`ParallelConfig` is a subclass of `RunConfig`. It introduces additional arguments to configure parallel training and enabling horizontal scaling of a single experiment, such as the number of trials, the maximum number of trials to run concurrently, the target metrics to optimize, and more.\n",
    "\n",
    "| Parameter             | Usage                                                                                                                                              |\n",
    "|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| total_trials          | total number of trials.                                                                                                                            |\n",
    "| concurrent_trials     | number of trials to run concurrently.                                                                                                              |\n",
    "| search_space          | search space for hyperparameter search,eg. ``{\"train_config.optimizer_config.arguments.lr\": SearchSpace(value_range=[0, 10], value_type=\"int\"),}`` |\n",
    "| optim_metrics         | metrics to optimize, eg. ``{\"val_loss\": \"min\"}``                                                                                                   |\n",
    "| search_algo           | type of search algorithm.                                                                                                                          |\n",
    "| ignore_invalid_params | whether to ignore invalid parameters when sampling.                                                                                                |\n",
    "| remote_config         | remote storage configuration.                                                                                                                      |\n",
    "| gcp_config            | gcp configuration.                                                                                                                                 |\n",
    "| gpu_mb_per_experiment | gpu resource to assign to an experiment.                                                                                                           |\n",
    "| cpus_per_experiment   | cpu resource to assign to an experiment.                                                                                                           |\n",
    "\n",
    "It's worth to mention `search_space`, which is used to define a set of continuous or categorical/discrete values for a certain hyperparameter that you want to ablate. Refer to [Search Space basics](./Search-space-tutorial.ipynb) to learn more about how to use it for ablation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelConfig\n",
    "\n",
    "This configuration can be used to add parameters specific to the model you're using. A sample use case for this is when you want to try different model sizes, number of layers, activation functions, etc. You can do this by creating a custom `ModelConfig` class for the model and include these parameters. One advantage of this is that `ablator` will be able to create a search space over the parameters and then run Hyperparameter optimization.\n",
    "\n",
    "There are 2 steps that are required after defining a custom model config class for your model:\n",
    "\n",
    "- Pass the custom config to its constructor so you can construct the model using the parameters that's defined in the custom config.\n",
    "\n",
    "- Create a custom running config class (decorated with `configclass` decorator), to update `model_config` argument to proper type, e.g `MyCustomModelConfig` (since `model_config` attribute of the running configuration, `RunConfig` or `ParallelConfig`, is originally of type `ModelConfig`).\n",
    "\n",
    "Note that in the model config class, arguments can be defined as **Stateless** or **Derived** data type. These are custom Python annotations to define attributes to which the experiment state is agnostic.\n",
    "\n",
    "- **Stateless** is used if a variable can take different value assignments between trials or experiments. For example, the learning rate, as we can resume training a model with different learning rates, should be stateless. Note that if you're declaring a variable to be Stateless, it must be assigned an initial value before launching the experiment.\n",
    "\n",
    "- **Derived** attributes are **Stateful** and are un-decided at the start of the experiment. Their values are determined by internal experiment processes that can depend on other experimental attributes, e.g model input size that depends on the dataset.\n",
    "\n",
    "- **Stateful** is opposite to **Stateless**, i.e its value must be the same between different experiments. For example, when you continue training a paused model, the model architecture should be the same (number of layers, output size). Stateful variables, defined as a primitive datatype, are required at initialization.\n",
    "\n",
    "Below is an example of a simple 1-layer neural network model, with configuration for input size (to be inferred); hidden layer dimension, activation function, and dropout rate (all of which are stateful); learning rate (stateless)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from ablator import RunConfig, ModelConfig, Stateless, Derived, configclass\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MyModelConfig(ModelConfig):\n",
    "    inp_size: Derived[int]\n",
    "    lr: Stateless[float]\n",
    "    hidden_dim: int\n",
    "    activation: str\n",
    "    dropout: float\n",
    "\n",
    "@configclass\n",
    "class CustomRunConfig(RunConfig):\n",
    "    model_config: MyModelConfig\n",
    "\n",
    "class MyCustomModel(nn.Module):\n",
    "    def __init__(self, config: MyModelConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(config.inp_size, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        if config.activation == \"relu\":\n",
    "            self.activate = nn.ReLU()\n",
    "        elif config.activation == \"elu\":\n",
    "            self.activate = nn.ELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        out = self.linear(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.activate(out)\n",
    "\n",
    "        return {\"preds\": out, \"labels\": out}, x.sum().abs()\n",
    "\n",
    "model_config = MyModelConfig(lr=0.01, hidden_dim=100, activation=\"relu\", dropout=0.3)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrainConfig\n",
    "\n",
    "This configuration class defines everything that is related to the main training process of your model, which includes dataset name, batch size, number of epochs, optimizer, scheduler. 2 important attributes to metion are `optimizer_config` and `scheduler_config`. As the names suggest, they configure the optimizer and scheduler to be used in the training process.\n",
    "\n",
    "| Parameter         | Usage                                                                 |\n",
    "|-------------------|-----------------------------------------------------------------------|\n",
    "| dataset           | dataset name. maybe used in custom dataset loader functions.          |\n",
    "| batch_size        | batch size.                                                           |\n",
    "| epochs            | number of epochs to train.                                            |\n",
    "| optimizer_config  | optimizer configuration. (check ``OptimizerConfig`` for more details) |\n",
    "| scheduler_config  | scheduler configuration. (check ``SchedulerConfig`` for more details) |\n",
    "| rand_weights_init | whether to initialize model weights randomly.                         |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OptimizerConfig and SchedulerConfig\n",
    "\n",
    "`OptimizerConfig` is a config class that allows user choose the optimizer they wanted. Currently, we supports SGD optimizer, Adam optimizer, and AdamW optimizer.\n",
    "\n",
    "`SchedulerConfig`, on the other hand, can be used for scheduling learning rate updates in the training process.\n",
    "\n",
    "Both of these config classes have similar arguments:\n",
    "\n",
    "| Parameter | Usage                                                                                                                                                                       |\n",
    "|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| name      | The type of the scheduler or optimizer, this can be any in ``['None', 'step', 'cycle', 'plateau']``<br> for schedulers and in ``['sgd', 'adam', 'adamw']`` for optimizers. |\n",
    "| arguments | The arguments for the scheduler or optimizer, specific to a certain type of scheduler or scheduler.                                                                         |\n",
    "\n",
    "The table below shows how arguments can be defined for each type of optimzer:\n",
    "\n",
    "| Optimizer type | Arguments                                                                                                                                                                                                                                                                           |\n",
    "|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| sgd            | `weight_decay` : Weight decay rate<br> `momentum` : Momentum factor<br>                                                                                                                                                                                                           |\n",
    "| adam           | `betas` : Coefficients for computing running averages of gradient and its square (default is ``(0.5, 0.9)``).<br/> `weight_decay` : Weight decay rate (default is ``0.0``).<br/>                                                                                                    |\n",
    "| adamw          | `betas` : Coefficients for computing running averages of gradient and its square (default is ``(0.9, 0.999)``).<br/> `eps` : Term added to the denominator to improve numerical stability (default is ``1e-8``).<br/> `weight_decay` : Weight decay rate (default is ``0.0``).<br/> |\n",
    "\n",
    "The table below shows how arguments can be defined for each type of scheduler:\n",
    "\n",
    "| Scheduler type | Arguments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "|----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| cycle          | `max_lr` : Upper learning rate boundaries in the cycle.<br> `total_steps` : The total number of steps to run the scheduler in a cycle.<br> `step_when` : The step type at which the `scheduler.step()` should be invoked: ``'train'``, ``'val'``, or ``'epoch'``.             |\n",
    "| plataeu        | `patience` : Number of epochs with no improvement after which learning rate will be reduced.<br> `min_lr` : A lower bound on the learning rate. `mode` : One of ``'min'``, ``'max'``, or ``'auto'``, which defines the<br>direction of optimization, so as to adjust the learning rate accordingly, i.e when a certain metric ceases improving.<br> `factor` : Factor by which the learning rate will be reduced. ``new_lr = lr * factor``.<br> `threshold` : Threshold for measuring the new optimum, to only focus on significant changes.<br> `verbose` : If ``True``, prints a message to ``stdout`` for each update.<br> `step_when` : The step type at which the scheduler should be invoked: ``'train'``, ``'val'``, or ``'epoch'``.<br> |\n",
    "| step           | `step_size` : Period of learning rate decay, by default 1.<br> `gamma` : Multiplicative factor of learning rate decay, by default 0.99.<br> `step_when` : The step type at which the scheduler should be invoked: ``'train'``, ``'val'``, or ``'epoch'``.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
    "\n",
    "The following code snippet describes how to initialize these objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from ablator import OptimizerConfig, SchedulerConfig\n",
    "\n",
    "optimizer_config = OptimizerConfig(name=\"sgd\", arguments={\"lr\": 0.1})\n",
    "scheduler_config = SchedulerConfig(name=\"cycle\", arguments={\"max_lr\": 0.5, \"total_steps\": 50})\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine everything. Ablator trainer requires a model wrapper and a running config when initializing, after that, experiment can be launched via `trainer.launch()`. Note that this tutorial only focuses on defining the running configuration `run_config`, for Ablator trainer, refer to [Prototyping models](./Prototyping-models.ipynb) and [HPO](./HPO-tutorial.ipynb).\n",
    "\n",
    "Take the code snippet below as an example, `train_config` sets up the dataset, batch size, epochs, and references the optimizer configuration and scheduler configuration. Next, `config` object combines the `train_config` and `model_config`, along with runtime settings like verbosity and device."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from ablator import TrainConfig\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    dataset=\"test\",\n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    optimizer_config=optimizer_config,\n",
    "    scheduler_config=scheduler_config,\n",
    ")\n",
    "\n",
    "config = CustomRunConfig(\n",
    "    train_config=train_config,\n",
    "    model_config=MyModelConfig(),\n",
    "    verbose=\"silent\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the configuration created, we are half-way to running ablation experiment with the ablator trainer.\n",
    "```\n",
    "trainer = ParallelTrainer(wrapper=model_wrapper, run_config=run_config)\n",
    "trainer.launch()\n",
    "```\n",
    "In the next chapter, you will learn how to create the model wrapper, the other half that's left. We will start with training a single model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different methods to define running configurations\n",
    "\n",
    "There are 3 ways to provide values to the configurations: named arguments, file-based, or dictionary-based. All examples from the previous sections are actually the named arguments method. Now let's look at how file based method and dictionary based method work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File-based\n",
    "\n",
    "File based configuration is a way for you to create simple configuration files, passing configuration values to a single `yaml` file. After that, based on the type of running configuration you want, you can use `RunConfigClass.load(path/to/yaml/file)` method to create configuration with values provided in the config file.\n",
    "\n",
    "To write these config files, simply follow `key : value` syntax (each pair on a single line). The following example shows what a config yaml file looks like. We will name it `config.yaml`:\n",
    "```\n",
    "experiment_dir: \"/tmp/dir\"\n",
    "train_config:\n",
    "  dataset: test\n",
    "  batch_size: 128\n",
    "  epochs: 2\n",
    "  optimizer_config:\n",
    "    name: sgd\n",
    "    arguments:\n",
    "      lr: 0.1\n",
    "  scheduler_config:\n",
    "    name: cycle\n",
    "    arguments:\n",
    "      max_lr: 0.5\n",
    "      total_steps: 50\n",
    "model_config:\n",
    "  inp_size: 50\n",
    "  hidden_dim: 100\n",
    "  activation: \"relu\"\n",
    "  dropout: 0.15\n",
    "verbose: \"silent\"\n",
    "device: \"cpu\"\n",
    "```\n",
    "We can see that the outermost arguments are from `RunConfig`. Also note how `train_config`, which corresponds to `TrainConfig` object in the running config, has its arguments defined 1 level below (indented). Therefore, the first rule to follow is that the arguments to use are from the running config class, either `RunConfig` or `ParallelConfig`, so make sure you use the right set of arguments. The second rule is that any arguments that is another config class should be indented 1 level from its parent config class.\n",
    "\n",
    "Now in your code, only 1 single line of code is required to load these values to create the config object:\n",
    "```\n",
    "config = CustomRunConfig.load(\"path/to/yaml/file\")\n",
    "```\n",
    "\n",
    "Note that since we created a custom running configuration class `CustomRunConfig` that is tied to the custom model config in the previous sections, we used `CustomRunConfig.load(\"path/to/yaml/file\")` to load configuration from file. Otherwise, if you're not creating any subclasses, `RunConfig.load(\"path\")` or `ParallelConfig.load(\"path\")` also works."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary based\n",
    "\n",
    "Another alternative is similar to the file-based method, but it's defining configurations in a dictionary instead of a yaml file, and then the dictionary will be passed (as keyword arguments) to the running configuration at initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "configuration = {\n",
    "    \"experiment_dir\": \"/tmp/dir\",\n",
    "    \"train_config\": {\n",
    "        \"dataset\": \"test\",\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\": 2,\n",
    "        \"optimizer_config\":{\n",
    "            \"name\": \"sgd\",\n",
    "            \"arguments\": {\n",
    "                \"lr\": 0.1\n",
    "            }\n",
    "        },\n",
    "        \"scheduler_config\":{\n",
    "            \"name\": \"cycle\",\n",
    "            \"arguments\":{\n",
    "                \"max_lr\": 0.5,\n",
    "                \"total_steps\": 50\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"model_config\": {\n",
    "        \"inp_size\": 50,\n",
    "        \"hidden_dim\": 100,\n",
    "        \"activation\": \"relu\",\n",
    "        \"dropout\": 0.15\n",
    "    },\n",
    "    \"verbose\": \"silent\",\n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "config = CustomRunConfig(\n",
    "    **configuration\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to define running configurations, you can start creating your own prototype. In the next chapter, we will learn how to write a prototype for your model, combine it with the running configuration, and launch the experiment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
